{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np, scipy, matplotlib.pyplot as plt\n",
    "import librosa, librosa.display\n",
    "from scipy import signal\n",
    "import pandas as pd\n",
    "import mir_eval\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence, detect_nonsilent\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of audio files: 4\n",
      "85SM_2020-01-27_14-40-03.wav\n",
      "87SM_2020-01-27_15-43-59.wav\n",
      "89SF_2020-01-27_16-51-22.wav\n",
      "91SM_2020-01-30_10-20-53.wav\n",
      "Chick ID: 85 87 89 91\n"
     ]
    }
   ],
   "source": [
    "data_root = 'chickcall_dataset/'\n",
    "wav_files = [] \n",
    "with open('file_names.txt', 'r') as f:\n",
    "    Lines = f.readlines()\n",
    "    for line in Lines: \n",
    "        wav_files.append(line.strip())\n",
    "        \n",
    "print('Number of audio files:', format(len(wav_files)))\n",
    "print(*wav_files, sep='\\n')\n",
    "\n",
    "# chick call annotation files\n",
    "csv_files = [file.replace('.wav', '.csv') for file in wav_files]\n",
    "chickID = [int(wav_files[k][:2]) for k in range(len(wav_files))]\n",
    "print('Chick ID:', *chickID, sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onset detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "window = 0.15\n",
    "n_fft = 2048 * 2\n",
    "hop_length = 1024 // 2\n",
    "lag = 3 \n",
    "n_mels = 8\n",
    "fmin = 2048 \n",
    "fmax = 6000 \n",
    "max_size = 18 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir onset_detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca6a2bdfbde49f99e5fafe35d993a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chick ID: 85\n",
      "Precision, recall, F-measure: 0.743268 0.934537 0.828000\n",
      "Total number of annotated onsets:  443\n",
      "Chick ID: 87\n",
      "Precision, recall, F-measure: 0.949884 0.981673 0.965517\n",
      "Total number of annotated onsets:  1255\n",
      "Chick ID: 89\n",
      "Precision, recall, F-measure: 0.879369 0.988593 0.930788\n",
      "Total number of annotated onsets:  789\n",
      "Chick ID: 91\n",
      "Precision, recall, F-measure: 0.769578 0.971483 0.858824\n",
      "Total number of annotated onsets:  526\n"
     ]
    }
   ],
   "source": [
    "PRF_onset = []\n",
    "\n",
    "for k in tqdm(range(len(wav_files))):\n",
    "    \n",
    "    chick = wav_files[k][:2]\n",
    "    sr, y = scipy.io.wavfile.read(data_root + wav_files[k])\n",
    "    y = (y[:,0] + y[:,1])/2\n",
    "    y /= np.max(np.abs(y))\n",
    "\n",
    "    # ground truth\n",
    "    truth = pd.read_csv(data_root + csv_files[k])\n",
    "    truth_times = np.zeros(len(truth) // 2)\n",
    "    truth_times[::1] = truth['time'][0::2]\n",
    "    np.savetxt('onset_detect/reference_chick' + chick + '.txt', truth_times, delimiter=',')\n",
    "\n",
    "    S = librosa.feature.melspectrogram(y, sr=sr, n_fft=n_fft,\n",
    "                                       hop_length=hop_length,\n",
    "                                       fmin=fmin,\n",
    "                                       fmax=fmax,\n",
    "                                       n_mels=n_mels)\n",
    "\n",
    "    odf_sf = librosa.onset.onset_strength(S=librosa.power_to_db(S, ref=np.max),\n",
    "                                          sr=sr,\n",
    "                                          hop_length=hop_length,\n",
    "                                          lag=lag, max_size=max_size)\n",
    "\n",
    "    onset_sf = librosa.onset.onset_detect(onset_envelope=odf_sf,\n",
    "                                          sr=sr,\n",
    "                                          hop_length=hop_length,\n",
    "                                          units='time', backtrack=True)\n",
    "    np.savetxt('onset_detect/estimated_chick' + chick + '.txt', onset_sf, delimiter=',')\n",
    "\n",
    "    reference_onsets = mir_eval.io.load_events('onset_detect/reference_chick' + chick + '.txt')\n",
    "    estimated_onsets = mir_eval.io.load_events('onset_detect/estimated_chick' + chick + '.txt')\n",
    "    scores = mir_eval.onset.evaluate(reference_onsets, estimated_onsets)\n",
    "\n",
    "    FPR = list(mir_eval.onset.f_measure(reference_onsets, estimated_onsets, window=window))\n",
    "    PRF_onset.append([int(chick)] + [FPR[1], FPR[2], FPR[0]])\n",
    "    print('Chick ID: ' + chick)\n",
    "    print('Precision, recall, F-measure: %f %f %f' % (FPR[1], FPR[2], FPR[0]))\n",
    "    print('Total number of annotated onsets: ', len(reference_onsets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Averafe precision, recall, F-measure: [83.5  96.75 89.75]\n"
     ]
    }
   ],
   "source": [
    "PRF_onset = np.round(np.array(PRF_onset),2)\n",
    "print('Averafe precision, recall, F-measure: {:}'.format(np.round(np.mean(PRF_onset, axis=0)[1:]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimated segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = {k:[] for k in range(len(wav_files))}\n",
    "\n",
    "for k in range(len(wav_files)):\n",
    "    chick = wav_files[k][:2]\n",
    "    estimated_onsets = mir_eval.io.load_events('onset_detect/estimated_chick' + chick + '.txt')\n",
    "    segs[k] = np.zeros((len(estimated_onsets),2)) # start, end (in s)\n",
    "    for m in range(len(estimated_onsets)-1):\n",
    "        segs[k][m,:] = np.array([estimated_onsets[m], estimated_onsets[m+1]])\n",
    "    segs[k][m+1,:] = np.array([estimated_onsets[m+1], len(AudioSegment.from_wav(data_root + wav_files[k]))/1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reference segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = {k:[] for k in range(len(wav_files))}\n",
    "seg_dur = []\n",
    "\n",
    "for k in range(len(wav_files)):\n",
    "    referece_pd = pd.read_csv(data_root + csv_files[k])\n",
    "    reference_times = referece_pd['time']\n",
    "    reference[k] = np.zeros((len(reference_times)//2,2))\n",
    "    reference[k][:,0] = reference_times[::2]\n",
    "    reference[k][:,1] = reference_times[1::2]\n",
    "    seg_dur.append(min(reference[k][:,1] - reference[k][:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## silence removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_silence_len': 5, 'silence_thresh': -28}\n",
      "{'min_silence_len': 5, 'silence_thresh': -30}\n",
      "{'min_silence_len': 5, 'silence_thresh': -32}\n",
      "{'min_silence_len': 5, 'silence_thresh': -34}\n",
      "{'min_silence_len': 10, 'silence_thresh': -28}\n",
      "{'min_silence_len': 10, 'silence_thresh': -30}\n",
      "{'min_silence_len': 10, 'silence_thresh': -32}\n",
      "{'min_silence_len': 10, 'silence_thresh': -34}\n"
     ]
    }
   ],
   "source": [
    "# grid search for silence removal params\n",
    "param_grid = {'min_silence_len': [5, 10], 'silence_thresh': [-28, -30, -32, -34]}\n",
    "print(*list(ParameterGrid(param_grid)), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ea9828ae3a480a915f0d5d64410355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "F_frame_grid = []\n",
    "F_event_grid = []\n",
    "\n",
    "for param in tqdm(ParameterGrid(param_grid)):\n",
    "\n",
    "    seg_new = {k:np.zeros((1,2)) for k in range(len(wav_files))}\n",
    "    seg_tosave = {k:[] for k in range(len(wav_files))}\n",
    "    # pydub is in milliseconds\n",
    "    for k in range(len(wav_files)):\n",
    "        sound = AudioSegment.from_wav(data_root+wav_files[k])\n",
    "        for m in range(len(segs[k])):\n",
    "            seg_audio = sound[int(segs[k][m,0]*1000):int(segs[k][m,1]*1000)]\n",
    "            chunks = detect_nonsilent(seg_audio, min_silence_len=param['min_silence_len'], \n",
    "                                      silence_thresh=param['silence_thresh'])\n",
    "            # select the one with the maximum duration\n",
    "            if chunks != []:\n",
    "                chunk_len_all = []\n",
    "                for n in range(len(chunks)):\n",
    "                    chunk_len_all.append(chunks[n][1] - chunks[n][0])\n",
    "                max_idx = chunk_len_all.index(max(chunk_len_all))\n",
    "                if chunks[max_idx][1] - chunks[max_idx][0] > 50:  # > 50ms\n",
    "                    seg_new[k] = np.vstack((seg_new[k], np.array([chunks[max_idx][0]/1000, chunks[max_idx][1]/1000])+segs[k][m,0]))   \n",
    "                    seg_tosave[k].extend([chunks[max_idx][0]/1000+segs[k][m,0], chunks[max_idx][1]/1000+segs[k][m,0]])\n",
    "        seg_new[k] = seg_new[k][1:]\n",
    "        \n",
    "    ###### event-based evaluation ######\n",
    "    # evaluate refined segs: onset + duration\n",
    "    PRF = np.zeros((4,3))\n",
    "    for m in range(4):\n",
    "        truthAll = reference[m]\n",
    "        predAll = seg_new[m]\n",
    "\n",
    "        matched1 = mir_eval.util.match_events(truthAll[:,0], predAll[:,0],window=.2, distance=None)\n",
    "        label=1\n",
    "\n",
    "        dur = .5 # at least 50% duration overlapped\n",
    "        # check each one on the duration\n",
    "        TP = 0\n",
    "        for k in range(len(matched1)):\n",
    "            interval_truth = pd.Interval(truthAll[matched1[k][0],0], truthAll[matched1[k][0],1])\n",
    "            interval_pred = pd.Interval(predAll[matched1[k][1],0], predAll[matched1[k][1],1])\n",
    "            if interval_truth.overlaps(interval_pred):\n",
    "                time_sorted = np.sort([truthAll[matched1[k][0],0], truthAll[matched1[k][0],1], \n",
    "                      predAll[matched1[k][1],0], predAll[matched1[k][1],1]]) # start, end, start, end\n",
    "                event_dur = time_sorted[2] - time_sorted[1]\n",
    "                if event_dur / (truthAll[matched1[k][0],1] - truthAll[matched1[k][0],0]) > dur:\n",
    "                     TP += 1\n",
    "\n",
    "        FN = len(truthAll) - TP\n",
    "        FP = len(predAll) - TP\n",
    "\n",
    "        P1_event = TP / (TP + FP) * 100; R1_event = TP / (TP + FN) * 100; \n",
    "        F1_event = 2 * P1_event * R1_event / (P1_event + R1_event)\n",
    "        PRF[m,:] = np.array([P1_event,R1_event,F1_event])\n",
    "        \n",
    "    F_event_grid.append(np.mean(PRF,0)[-1])\n",
    "    \n",
    "    ###### frame-based evaluation ######\n",
    "    # event back to frame, use frame size of onset detection: hop_length = 1024 // 2 \n",
    "    F_frame = []\n",
    "\n",
    "    for k in range(len(wav_files)):\n",
    "        sr, y = scipy.io.wavfile.read(data_root + wav_files[k])\n",
    "        frameNum = int(len(y)/hop_length)\n",
    "\n",
    "        label_reference = np.zeros((frameNum), dtype=int)\n",
    "\n",
    "        for m in range(len(reference[k])):\n",
    "            start_sam = int(reference[k][m,0] * sr / hop_length)\n",
    "            end_sam = int(reference[k][m,1] * sr / hop_length)\n",
    "            label_reference[start_sam:end_sam] = np.zeros((end_sam-start_sam), dtype=int) + 1\n",
    "\n",
    "        label_detect = np.zeros((frameNum), dtype=int)\n",
    "\n",
    "        for m in range(len(seg_new[k])):\n",
    "            start_sam = int(seg_new[k][m,0] * sr / hop_length)\n",
    "            end_sam = int(seg_new[k][m,1] * sr / hop_length)\n",
    "            label_detect[start_sam:end_sam] = np.zeros((end_sam-start_sam), dtype=int) + 1\n",
    "\n",
    "        report = pd.DataFrame(classification_report(label_reference, label_detect, output_dict=True))\n",
    "        F_frame.append([report['1']['precision'], report['1']['recall'], report['1']['f1-score']])\n",
    "\n",
    "    F_frame = np.array(F_frame)\n",
    "    F_frame_grid.append(np.mean(F_frame,0)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_silence_len': 5, 'silence_thresh': -32}\n"
     ]
    }
   ],
   "source": [
    "aver_F = (np.array(F_frame_grid)*100 + np.array(F_event_grid)) / 2\n",
    "idx = np.argmax(aver_F)\n",
    "param = list(ParameterGrid(param_grid))[idx]\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save detected segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_new = {k:np.zeros((1,2)) for k in range(len(wav_files))}\n",
    "seg_tosave = {k:[] for k in range(len(wav_files))}\n",
    "# pydub is in milliseconds\n",
    "for k in range(len(wav_files)):\n",
    "    sound = AudioSegment.from_wav(data_root+wav_files[k])\n",
    "    for m in range(len(segs[k])):\n",
    "        seg_audio = sound[int(segs[k][m,0]*1000):int(segs[k][m,1]*1000)]\n",
    "        chunks = detect_nonsilent(seg_audio, min_silence_len=param['min_silence_len'], \n",
    "                                  silence_thresh=param['silence_thresh'])\n",
    "        # select the one with the maximum duration\n",
    "        if chunks != []:\n",
    "            chunk_len_all = []\n",
    "            for n in range(len(chunks)):\n",
    "                chunk_len_all.append(chunks[n][1] - chunks[n][0])\n",
    "            max_idx = chunk_len_all.index(max(chunk_len_all))\n",
    "            if chunks[max_idx][1] - chunks[max_idx][0] > 50:  # > 50ms\n",
    "                seg_new[k] = np.vstack((seg_new[k], np.array([chunks[max_idx][0]/1000, chunks[max_idx][1]/1000])+segs[k][m,0]))   \n",
    "                seg_tosave[k].extend([chunks[max_idx][0]/1000+segs[k][m,0], chunks[max_idx][1]/1000+segs[k][m,0]])\n",
    "    seg_new[k] = seg_new[k][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(548, 2) (1244, 2) (871, 2) (652, 2)\n",
      "(557, 2) (1297, 2) (887, 2) (664, 2)\n",
      "(443, 2) (1255, 2) (789, 2) (526, 2)\n"
     ]
    }
   ],
   "source": [
    "np.savez('refined_segs.npz',seg_new[0],seg_new[1],seg_new[2],seg_new[3])\n",
    "np.savez('estimated_segs.npz',segs[0],segs[1],segs[2],segs[3])\n",
    "\n",
    "print(seg_new[0].shape,seg_new[1].shape,seg_new[2].shape,seg_new[3].shape)\n",
    "print(segs[0].shape,segs[1].shape,segs[2].shape,segs[3].shape)\n",
    "print(reference[0].shape, reference[1].shape, reference[2].shape, reference[3].shape, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## event-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate refined segs: onset + duration\n",
    "PRF = np.zeros((4,3))\n",
    "for m in range(4):\n",
    "    truthAll = reference[m]\n",
    "    predAll = seg_new[m]\n",
    "\n",
    "    matched1 = mir_eval.util.match_events(truthAll[:,0], predAll[:,0],window=.2, distance=None)\n",
    "    label=1\n",
    "\n",
    "    dur = .5 # at least 50% duration overlapped\n",
    "    # check each one on the duration\n",
    "    TP = 0\n",
    "    for k in range(len(matched1)):\n",
    "        interval_truth = pd.Interval(truthAll[matched1[k][0],0], truthAll[matched1[k][0],1])\n",
    "        interval_pred = pd.Interval(predAll[matched1[k][1],0], predAll[matched1[k][1],1])\n",
    "        if interval_truth.overlaps(interval_pred):\n",
    "            time_sorted = np.sort([truthAll[matched1[k][0],0], truthAll[matched1[k][0],1], \n",
    "                  predAll[matched1[k][1],0], predAll[matched1[k][1],1]]) # start, end, start, end\n",
    "            event_dur = time_sorted[2] - time_sorted[1]\n",
    "            if event_dur / (truthAll[matched1[k][0],1] - truthAll[matched1[k][0],0]) > dur:\n",
    "                 TP += 1\n",
    "\n",
    "    FN = len(truthAll) - TP\n",
    "    FP = len(predAll) - TP\n",
    "\n",
    "    P1_event = TP / (TP + FP) * 100; R1_event = TP / (TP + FN) * 100; \n",
    "    F1_event = 2 * P1_event * R1_event / (P1_event + R1_event)\n",
    "    PRF[m,:] = np.array([P1_event,R1_event,F1_event])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for each chick:\n",
      "[62. 77. 69.]\n",
      "[68. 67. 67.]\n",
      "[78. 86. 81.]\n",
      "[65. 81. 72.]\n",
      "Average F-measure:\n",
      "[68.25 77.75 72.25]\n"
     ]
    }
   ],
   "source": [
    "PRF = np.round(PRF)\n",
    "print('P, R, F for each chick:')\n",
    "print(*PRF, sep='\\n')\n",
    "print('Average F-measure:')\n",
    "print(np.round(np.mean(PRF,0),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## frame-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event back to frame, use frame size of onset detection: hop_length = 1024 // 2 \n",
    "F_frame = []\n",
    "\n",
    "for k in range(len(wav_files)):\n",
    "    sr, y = scipy.io.wavfile.read(data_root + wav_files[k])\n",
    "    frameNum = int(len(y)/hop_length)\n",
    "    \n",
    "    label_reference = np.zeros((frameNum), dtype=int)\n",
    "\n",
    "    for m in range(len(reference[k])):\n",
    "        start_sam = int(reference[k][m,0] * sr / hop_length)\n",
    "        end_sam = int(reference[k][m,1] * sr / hop_length)\n",
    "        label_reference[start_sam:end_sam] = np.zeros((end_sam-start_sam), dtype=int) + 1\n",
    "        \n",
    "    label_detect = np.zeros((frameNum), dtype=int)\n",
    "\n",
    "    for m in range(len(seg_new[k])):\n",
    "        start_sam = int(seg_new[k][m,0] * sr / hop_length)\n",
    "        end_sam = int(seg_new[k][m,1] * sr / hop_length)\n",
    "        label_detect[start_sam:end_sam] = np.zeros((end_sam-start_sam), dtype=int) + 1\n",
    "        \n",
    "    report = pd.DataFrame(classification_report(label_reference, label_detect, output_dict=True))\n",
    "    F_frame.append([report['1']['precision'], report['1']['recall'], report['1']['f1-score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P, R, F for each chick:\n",
      "[56. 80. 66.]\n",
      "[75. 70. 72.]\n",
      "[87. 82. 84.]\n",
      "[71. 81. 75.]\n",
      "Average F-measure:\n",
      "[72.25 78.25 74.25]\n"
     ]
    }
   ],
   "source": [
    "F_frame = np.round(np.array(F_frame), 2)\n",
    "print('P, R, F for each chick:')\n",
    "print(*F_frame*100, sep='\\n')\n",
    "print('Average F-measure:')\n",
    "print(np.round(np.mean(F_frame, axis=0)*100,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185.806px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
